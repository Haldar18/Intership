{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a454edd3",
   "metadata": {},
   "source": [
    "### 1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e498f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7fdb420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_1636\\2226573296.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb44cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d227eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the input:Guitar\n"
     ]
    }
   ],
   "source": [
    "val=input('Enter the input:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ee7d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user input\n",
    "lap=driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "lap.send_keys(val)\n",
    "\n",
    "#search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a94838",
   "metadata": {},
   "source": [
    "### 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e349c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url=[]\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//a[@class=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]')\n",
    "    for i in url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "    next_button=driver.find_elements(By.XPATH,\"/html/body/div[1]/div[2]/div[1]/div[1]/div/span[1]/div[1]/div[64]/div/div/span/a[3]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "186c3f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_name=[]\n",
    "Name=[]\n",
    "Price=[]\n",
    "Return_exchange=[]\n",
    "Expected_Delivery=[]\n",
    "Availability=[]\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    #For Brand name\n",
    "    try:\n",
    "        brand=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[5]/div[3]/div[4]/div[45]/div/table/tbody/tr[1]/td[2]/span')\n",
    "        Brand_name.append(brand.text)\n",
    "    except NoSuchElementException :\n",
    "        Brand_name.append('-')\n",
    "    \n",
    "     #For Product name\n",
    "    try:\n",
    "        product=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[5]/div[3]/div[4]/div[1]/div/h1/span')\n",
    "        Name.append(product.text)\n",
    "    except NoSuchElementException :\n",
    "        Name.append('-')\n",
    "    \n",
    "    #For Price\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[5]/div[3]/div[4]/div[10]/div[3]/div[1]/span[2]/span[2]/span[2]')\n",
    "        Price.append(price.text)\n",
    "    except NoSuchElementException :\n",
    "        Price.append('-')\n",
    "    \n",
    "     #For Return or exchange\n",
    "    try:\n",
    "        return_exchange=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[5]/div[3]/div[4]/div[22]/div[2]/div/div/div/div[2]/div/ol/li[3]/div/span/div[2]/span')\n",
    "        Return_exchange.append(return_exchange.text)\n",
    "    except NoSuchElementException :\n",
    "        Return_exchange.append('-')\n",
    "    \n",
    "     #For Expected Delivery\n",
    "    try:\n",
    "        delivery=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[5]/div[3]/div[1]/div[3]/div/div[1]/div/div/div/form/div/div/div/div/div[3]/div/div[2]/div[9]/div[1]/div/div/div/span/span[1]')\n",
    "        Expected_Delivery.append(delivery.text)\n",
    "    except NoSuchElementException :\n",
    "        Expected_Delivery.append('-')\n",
    "    \n",
    "    #For Availability\n",
    "    try:\n",
    "        avail=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[5]/div[3]/div[1]/div[3]/div/div[1]/div/div/div/form/div/div/div/div/div[3]/div/div[4]/div/div[1]/span')\n",
    "        Availability.append(avail.text)\n",
    "    except NoSuchElementException :\n",
    "        Availability.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b81d0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Brand Name':Brand_name,'Name of the Product':Name,'Price':Price,'Return/Exchange':Return_exchange,'Expected Delivery':Expected_Delivery,'Availability':Availability,'Product URL':product_url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b4c5191",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('GuitarDetails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8e412",
   "metadata": {},
   "source": [
    "### 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89c8e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b071143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_1636\\2226573296.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "699eccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://images.google.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "594e7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’.\n",
    "#For fruits\n",
    "#user input\n",
    "lap=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/textarea')\n",
    "lap.send_keys('fruits')\n",
    "\n",
    "#search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button/div/span')\n",
    "search.click()\n",
    "\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "images =driver.find_elements(By.XPATH,'//div[@class=\" bRMDJf islir\"]')\n",
    "\n",
    "img_urls=[]\n",
    "img_data=[]\n",
    "for image in images:\n",
    "    source=image.get_attribute('src')\n",
    "    if source is not None :\n",
    "        if(source[0:4]=='http'):\n",
    "            img_urls.append(source)\n",
    "\n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        breakBy.XPATH,\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response=requests.get(img_urls[i])\n",
    "    file=open(r\"C:\\Users\\lenovo\"+str(i)+\".jpg\",\"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc2d7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Cars\n",
    "#user input\n",
    "lap=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/textarea')\n",
    "lap.send_keys('Cars')\n",
    "\n",
    "#search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button/div/span')\n",
    "search.click()\n",
    "\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "images =driver.find_elements(By.XPATH,'//div[@class=\" bRMDJf islir\"]')\n",
    "\n",
    "img_urls=[]\n",
    "img_data=[]\n",
    "for image in images:\n",
    "    source=image.get_attribute('src')\n",
    "    if source is not None :\n",
    "        if(source[0:4]=='http'):\n",
    "            img_urls.append(source)\n",
    "\n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        breakBy.XPATH,\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response=requests.get(img_urls[i])\n",
    "    file=open(r\"C:\\Users\\lenovo\"+str(i)+\".jpg\",\"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c363ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Machine Learning\n",
    "#user input\n",
    "lap=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/textarea')\n",
    "lap.send_keys('Machine Learning')\n",
    "\n",
    "#search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button/div/span')\n",
    "search.click()\n",
    "\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "images =driver.find_elements(By.XPATH,'//div[@class=\" bRMDJf islir\"]')\n",
    "\n",
    "img_urls=[]\n",
    "img_data=[]\n",
    "for image in images:\n",
    "    source=image.get_attribute('src')\n",
    "    if source is not None :\n",
    "        if(source[0:4]=='http'):\n",
    "            img_urls.append(source)\n",
    "\n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        breakBy.XPATH,\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response=requests.get(img_urls[i])\n",
    "    file=open(r\"C:\\Users\\lenovo\"+str(i)+\".jpg\",\"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Guitar \n",
    "#user input\n",
    "lap=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/textarea')\n",
    "lap.send_keys('Guitar')\n",
    "\n",
    "#search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button/div/span')\n",
    "search.click()\n",
    "\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "images =driver.find_elements(By.XPATH,'//div[@class=\" bRMDJf islir\"]')\n",
    "\n",
    "img_urls=[]\n",
    "img_data=[]\n",
    "for image in images:\n",
    "    source=image.get_attribute('src')\n",
    "    if source is not None :\n",
    "        if(source[0:4]=='http'):\n",
    "            img_urls.append(source)\n",
    "\n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        breakBy.XPATH,\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response=requests.get(img_urls[i])\n",
    "    file=open(r\"C:\\Users\\lenovo\"+str(i)+\".jpg\",\"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5742eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Cakes\n",
    "#user input\n",
    "lap=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/textarea')\n",
    "lap.send_keys('Cakes')\n",
    "\n",
    "#search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button/div/span')\n",
    "search.click()\n",
    "\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "images =driver.find_elements(By.XPATH,'//div[@class=\" bRMDJf islir\"]')\n",
    "\n",
    "img_urls=[]\n",
    "img_data=[]\n",
    "for image in images:\n",
    "    source=image.get_attribute('src')\n",
    "    if source is not None :\n",
    "        if(source[0:4]=='http'):\n",
    "            img_urls.append(source)\n",
    "\n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        breakBy.XPATH,\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response=requests.get(img_urls[i])\n",
    "    file=open(r\"C:\\Users\\lenovo\"+str(i)+\".jpg\",\"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a05be",
   "metadata": {},
   "source": [
    "### Q4 Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923defea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f0b3833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_1636\\2226573296.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d443c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "355155dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user input\n",
    "lap=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "lap.send_keys('Oneplus Nord')\n",
    "\n",
    "#search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09f691e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for product urls\n",
    "product_url=[]\n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"_1fQZEK\"]')\n",
    "for i in url:\n",
    "    product_url.append(i.get_attribute('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd9e1f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand=[]\n",
    "Name=[]\n",
    "Color=[]\n",
    "Ram=[]\n",
    "Rom=[]\n",
    "Primary_camera=[]\n",
    "Secondary_camera=[]\n",
    "Display_size=[]\n",
    "Battery=[]\n",
    "Price=[]\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    #For Brand name\n",
    "    try:\n",
    "        brand=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[3]/div/div[1]/h1/span')\n",
    "        Brand.append(brand.text.split('N')[0])\n",
    "    except NoSuchElementException :\n",
    "        Brand.append('-')\n",
    "     #For Smartphone name\n",
    "    try:\n",
    "        name=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div[4]/div/div[2]/div/div[1]/table/tbody/tr[3]/td[2]/ul/li')\n",
    "        Name.append(name.text)\n",
    "    except NoSuchElementException :\n",
    "        Name.append('-')\n",
    "     #For Color name\n",
    "    try:\n",
    "        color=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div[4]/div/div[2]/div/div[1]/table/tbody/tr[4]/td[2]/ul/li')\n",
    "        Color.append(color.text)\n",
    "    except NoSuchElementException :\n",
    "        Color.append('-')\n",
    "     #For Ram\n",
    "    try:\n",
    "        ram=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[1]/div/div[2]/ul/li[1]')\n",
    "        Ram.append(ram.text.split('|')[0])\n",
    "    except NoSuchElementException :\n",
    "        Ram.append('-')\n",
    "    #For Rom\n",
    "    try:\n",
    "        rom=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[1]/div/div[2]/ul/li[1]')\n",
    "        Rom.append(rom.text.split('|')[1])\n",
    "    except NoSuchElementException :\n",
    "        Rom.append('-')\n",
    "     #For Primary Camera\n",
    "    try:\n",
    "        camera=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[1]/div/div[2]/ul/li[3]')\n",
    "        Primary_camera.append(camera.text.split('R')[0])\n",
    "    except NoSuchElementException :\n",
    "        Primary_camera.append('-')\n",
    "     #For Display_size\n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[1]/div/div[2]/ul/li[2]')\n",
    "        Display_size.append(size.text.split('D')[0])\n",
    "    except NoSuchElementException :\n",
    "        Display_size.append('-')\n",
    "     #For Battery\n",
    "    try:\n",
    "        battery=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[1]/div/div[2]/ul/li[4]')\n",
    "        Battery.append(battery.text)\n",
    "    except NoSuchElementException :\n",
    "        Battery.append('-')\n",
    "     #For Price\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[3]/div/div[4]/div[1]/div/div[1]')\n",
    "        Price.append(price.text)\n",
    "    except NoSuchElementException :\n",
    "        Price.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44f9cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Brand Name':Brand,'Smartphone Namet':Name,'Colour':Color,'Ram':Ram,'Storage':Rom,'Primary Camera':Primary_camera,'Display Size':Display_size,'Battery':Battery,'Price':Price,'Product URL':product_url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "418d21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('PhoneDetails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77526dd5",
   "metadata": {},
   "source": [
    "### 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d870cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4adaee93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_1636\\2226573296.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "50975cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We gonna search for delhi city on google maps\n",
    "driver.get('https://www.google.com/maps/place/Delhi/@28.6442874,76.7635802,10z/data=!3m1!4b1!4m6!3m5!1s0x390cfd5b347eb62d:0x37205b715389640!8m2!3d28.7040592!4d77.1024902!16zL20vMDlmMDc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21357c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    url_string=driver.current_url\n",
    "    print(\"Url Extracted:\",url_string)\n",
    "    lat_lng=re.findall(r'@(.*)data',url_string)\n",
    "except NoSuchElementException :\n",
    "        lat_lng.append('-')\n",
    "latitude=lat_lng.split(',')[0]\n",
    "longitude=lat_lng.split(',')[1].split(',')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24590463",
   "metadata": {},
   "source": [
    "### 7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "927eac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4a99d621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_1636\\2226573296.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "750af634",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.digit.in/top-products/best-gaming-laptops-40.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eb0af8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Laptop's urls\n",
    "product_url=[]\n",
    "url=driver.find_elements(By.XPATH,'//div[@class=\"specfpro_details_cont\"]/div/div/span[2]')\n",
    "for i in url:\n",
    "    product_url.append(i.get_attribute('data-href'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "517d333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand=[]\n",
    "Model=[]\n",
    "OS=[]\n",
    "Storage_type=[]\n",
    "Storage=[]\n",
    "Ram=[]\n",
    "Ram_type=[]\n",
    "\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    #For Brand name\n",
    "    try:\n",
    "        brand=driver.find_element(By.XPATH,'/html/body/div[7]/div[2]/div[2]')\n",
    "        Brand.append(brand.text.split(' ')[0])\n",
    "    except NoSuchElementException :\n",
    "        Brand.append('-')\n",
    "    #For Model name\n",
    "    try:\n",
    "        model=driver.find_element(By.XPATH,'//div[@class=\"remaining-details\"]/table/tbody/tr/td[3]')\n",
    "        Model.append(model.text)\n",
    "    except NoSuchElementException :\n",
    "        Model.append('-')\n",
    "     #For Operating System name\n",
    "    try:\n",
    "        os=driver.find_element(By.XPATH,'//div[@class=\"remaining-details\"]/table/tbody/tr[3]/td[3]')\n",
    "        OS.append(os.text)\n",
    "    except NoSuchElementException :\n",
    "        OS.append('-')\n",
    "      #For Storage Type name\n",
    "    try:\n",
    "        storage_type=driver.find_element(By.XPATH,'//div[@class=\"speci_details\"]/div[2]/div/table/tbody/tr/td[3]')\n",
    "        Storage_type.append(storage_type.text)\n",
    "    except NoSuchElementException :\n",
    "        Storage_type.append('-')\n",
    "     #For Storage\n",
    "    try:\n",
    "        storage=driver.find_element(By.XPATH,'//div[@class=\"speci_details\"]/div[2]/div/table/tbody/tr[2]/td[3]')\n",
    "        Storage.append(storage.text)\n",
    "    except NoSuchElementException :\n",
    "        Storage.append('-')\n",
    "\n",
    "      #For Ram Type name\n",
    "    try:\n",
    "        ram_type=driver.find_element(By.XPATH,'//div[@class=\"speci_details\"]/div/div[4]/table/tbody/tr[2]/td[3]')\n",
    "        Ram_type.append(ram_type.text)\n",
    "    except NoSuchElementException :\n",
    "        Ram_type.append('-')\n",
    "    #For Ram\n",
    "    try:\n",
    "        ram=driver.find_element(By.XPATH,'//div[@class=\"speci_details\"]/div/div[4]/table/tbody/tr/td[3]')\n",
    "        Ram.append(ram.text)\n",
    "    except NoSuchElementException :\n",
    "        Ram.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fcbc4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Brand':Brand,'Laptop Model':Model,'Operating System':OS,'Storage Type':Storage_type,'Storage':Storage,'Ram':Ram,'Ram Type':Ram_type})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f89f55",
   "metadata": {},
   "source": [
    "### 8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6d426409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "181b4813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_1636\\2226573296.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "746aa310",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.forbes.com/billionaires/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7eb0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank=[]\n",
    "Name=[]\n",
    "Net_Worth=[]\n",
    "Age=[]\n",
    "Citizenship=[]\n",
    "Source=[]\n",
    "Industry=[]\n",
    "#For Rank name\n",
    "    try:\n",
    "        rank=driver.find_element(By.XPATH,'//div[@class=\"Table_rank___YBhk Table_dataCell__2QCve\"]')\n",
    "        Rank.append(rank.text)\n",
    "    except NoSuchElementException :\n",
    "        Rank.append('-')\n",
    "#For name\n",
    "    try:\n",
    "        name=driver.find_element(By.XPATH,'//div[@class=\"Table_dataCell__2QCve\"]')\n",
    "        Name.append(name.text)\n",
    "    except NoSuchElementException :\n",
    "        Name.append('-')\n",
    "#For Net_Worth name\n",
    "    try:\n",
    "        net_worth=driver.find_element(By.XPATH,'//div[@class=\"Table_netWorth___L4R5 Table_dataCell__2QCve\"]')\n",
    "        Net_Worth.append(net_worth.text)\n",
    "    except NoSuchElementException :\n",
    "        Net_Worth.append('-')\n",
    "#For Age \n",
    "    try:\n",
    "        age=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[3]/div[2]/div[2]/div[2]/div[1]/div[1]/div[1]/div[2]/div[1]/div/div[4]/div')\n",
    "        Age.append(age.text)\n",
    "    except NoSuchElementException :\n",
    "        Age.append('-')\n",
    "#For Citizenship\n",
    "    try:\n",
    "        citizenship=driver.find_element(By.XPATH,'//div[@class=\"TableRow_cell__db-hv Table_cell__houv9\"]')\n",
    "        Citizenship.append(citizenship.text)\n",
    "    except NoSuchElementException :\n",
    "        Citizenship.append('-')\n",
    "#For Source \n",
    "    try:\n",
    "        source=driver.find_element(By.XPATH,'//div[@class=\"Table_dataCell__2QCve\"]')\n",
    "        Source.append(source.text)\n",
    "    except NoSuchElementException :\n",
    "        Source.append('-')\n",
    "#For Industry \n",
    "    try:\n",
    "        industry=driver.find_element(By.XPATH,'//*[@id=\"bernard-arnault\"]/div[7]/div')\n",
    "        Industry.append(industry.text)\n",
    "    except NoSuchElementException :\n",
    "        Industry.append('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a298dba7",
   "metadata": {},
   "source": [
    "### 9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a11ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0c74859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "126ecb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_1636\\2421393482.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")\n",
    "driver.get('https://www.youtube.com/watch?v=ajeTYqhRHno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "comment=[]\n",
    "upvote=[]\n",
    "time=[]\n",
    "comments =driver.find_elements(By.XPATH,'//yt-formatted-string[@id=\"content-text\"]') \n",
    "for i in comments[0:500]:\n",
    "    comment.append(i.text)\n",
    "\n",
    "up =driver.find_elements(By.XPATH,'//span[@id=\"vote-count-middle\"]') \n",
    "for i in up[0:500]:\n",
    "    upvote.append(i.text)\n",
    "\n",
    "t =driver.find_elements(By.XPATH,'//yt-formatted-string[@class=\"published-time-text style-scope ytd-comment-renderer\"]') \n",
    "for i in t[0:500]:\n",
    "    time.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ea673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame=({'Comments':comment,'Comment upvote':upvote,'time':time})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90fe40f",
   "metadata": {},
   "source": [
    "### 10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ff574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "36dcf175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_1636\\2705041224.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\lenovo\\Downloads\\chromedriver_win32\")\n",
    "driver.get('https://www.hostelworld.com/s?q=London,%20England&country=London&city=London&type=city&id=3&from=2023-05-20&to=2023-05-23&guests=2&page=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "60334a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for hostel urls\n",
    "hostel_url=[]\n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"property-card-container horizontal\"]')\n",
    "for i in url:\n",
    "    hostel_url.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hostel_name=[]\n",
    "Distance=[]\n",
    "Rating=[]\n",
    "Reviews=[]\n",
    "Property_Description=[]\n",
    "\n",
    "\n",
    "name=driver.find_element(By.XPATH,'//h2[@class=\"title title-6\"]/a')\n",
    "for i in distance:\n",
    "    Hostel_name.append(i.text)\n",
    "distance=driver.find_element(By.XPATH,'//a[@class=\"show-on-map\"]/span/span')\n",
    "for i in distance:\n",
    "    Distance.append(i.text)\n",
    "rating=driver.find_element(By.XPATH,'//div[@class=\"rating rating-summary-container big\"]/div')\n",
    "for i in rating:\n",
    "    Rating.append(i.text)\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    #For Brand name\n",
    "    try:\n",
    "        reviews=driver.find_element(By.XPATH,'//div[@class=\"summary orange\"]/div[2]')\n",
    "        Reviews.append(reviews.text)\n",
    "    except NoSuchElementException :\n",
    "        Reviews.append('-')\\\n",
    "    try:\n",
    "        des=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/section/div[6]/div/div[2]/div/div/div[2]')\n",
    "        Property_Description.append(des.text)\n",
    "    except NoSuchElementException :\n",
    "        Property_Description.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'hostel':Hostel_name,'Distance from city':Distance,'Rating':Rating,'Total Reviews':Reviews,'Property Description':Property_Description})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
